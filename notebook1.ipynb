{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "502 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-46-208.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>abc</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0060202050>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('s3://sec-finc/pivot_data_vpq/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cik: int, company_name: string, assigned_sic: int, accession_number_int: bigint, filing_date: date, document_fiscal_year_focus: int, datapoint_label: string, start_date: date, end_date: date, statement_type: string, segment_label: string, segment_hash: string, CostOfGoodsAndServicesSold: double, CostOfGoodsSold: double, CostOfServices: double, EarningsPerShareBasic: double, EarningsPerShareDiluted: double, GainLossOnDispositionOfAssets: double, GeneralAndAdministrativeExpense: double, IncomeTaxesPaid: double, IncreaseDecreaseInAccountsPayable: double, IncreaseDecreaseInAccountsReceivable: double, IncreaseDecreaseInAccruedLiabilities: double, IncreaseDecreaseInInventories: double, LaborAndRelatedExpense: double, NetCashProvidedByUsedInFinancingActivities: double, NetCashProvidedByUsedInFinancingActivitiesContinuingOperations: double, NetCashProvidedByUsedInInvestingActivities: double, NetCashProvidedByUsedInInvestingActivitiesContinuingOperations: double, NetCashProvidedByUsedInOperatingActivities: double, NetCashProvidedByUsedInOperatingActivitiesContinuingOperations: double, NetIncomeLoss: double, OperatingExpenses: double, OperatingIncomeLoss: double, PaymentsForRepurchaseOfCommonStock: double, PaymentsOfDividends: double, PaymentsOfDividendsCommonStock: double, PaymentsOfFinancingCosts: double, PaymentsToAcquireBusinessesNetOfCashAcquired: double, PaymentsToAcquirePropertyPlantAndEquipment: double, RepaymentsOfLongTermDebt: double, ResearchAndDevelopmentExpense: double, Revenues: double, SellingAndMarketingExpense: double, ShareBasedCompensation: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(477717, 45)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['cik',\n",
    " 'company_name',\n",
    " 'assigned_sic',\n",
    " 'accession_number_int',\n",
    " 'filing_date',\n",
    " 'document_fiscal_year_focus',\n",
    " 'datapoint_label',\n",
    " 'start_date',\n",
    " 'end_date',\n",
    " 'statement_type',\n",
    " 'segment_label',\n",
    " 'segment_hash',\n",
    " 'CostOfGoodsAndServicesSold',\n",
    " 'CostOfGoodsSold',\n",
    " 'CostOfServices',\n",
    " 'EarningsPerShareBasic',\n",
    " 'EarningsPerShareDiluted',\n",
    " 'GainLossOnDispositionOfAssets',\n",
    " 'GeneralAndAdministrativeExpense',\n",
    " 'IncomeTaxesPaid',\n",
    " 'IncreaseDecreaseInAccountsPayable',\n",
    " 'IncreaseDecreaseInAccountsReceivable',\n",
    " 'IncreaseDecreaseInAccruedLiabilities',\n",
    " 'IncreaseDecreaseInInventories',\n",
    " 'LaborAndRelatedExpense',\n",
    " 'NetCashProvidedByUsedInFinancingActivities',\n",
    " 'NetCashProvidedByUsedInFinancingActivitiesContinuingOperations',\n",
    " 'NetCashProvidedByUsedInInvestingActivities',\n",
    " 'NetCashProvidedByUsedInInvestingActivitiesContinuingOperations',\n",
    " 'NetCashProvidedByUsedInOperatingActivities',\n",
    " 'NetCashProvidedByUsedInOperatingActivitiesContinuingOperations',\n",
    " 'NetIncomeLoss',\n",
    " 'OperatingExpenses',\n",
    " 'OperatingIncomeLoss',\n",
    " 'PaymentsForRepurchaseOfCommonStock',\n",
    " 'PaymentsOfDividends',\n",
    " 'PaymentsOfDividendsCommonStock',\n",
    " 'PaymentsOfFinancingCosts',\n",
    " 'PaymentsToAcquireBusinessesNetOfCashAcquired',\n",
    " 'PaymentsToAcquirePropertyPlantAndEquipment',\n",
    " 'RepaymentsOfLongTermDebt',\n",
    " 'ResearchAndDevelopmentExpense',\n",
    " 'Revenues',\n",
    " 'SellingAndMarketingExpense',\n",
    " 'ShareBasedCompensation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('segment_label')\n",
    "df = df.drop('segment_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, count, when, col, isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['cik',\n",
    " 'company_name',\n",
    " 'assigned_sic',\n",
    " 'accession_number_int',\n",
    " 'document_fiscal_year_focus',\n",
    " 'datapoint_label',\n",
    " 'statement_type',\n",
    " 'CostOfGoodsAndServicesSold',\n",
    " 'CostOfGoodsSold',\n",
    " 'CostOfServices',\n",
    " 'EarningsPerShareBasic',\n",
    " 'EarningsPerShareDiluted',\n",
    " 'GainLossOnDispositionOfAssets',\n",
    " 'GeneralAndAdministrativeExpense',\n",
    " 'IncomeTaxesPaid',\n",
    " 'IncreaseDecreaseInAccountsPayable',\n",
    " 'IncreaseDecreaseInAccountsReceivable',\n",
    " 'IncreaseDecreaseInAccruedLiabilities',\n",
    " 'IncreaseDecreaseInInventories',\n",
    " 'LaborAndRelatedExpense',\n",
    " 'NetCashProvidedByUsedInFinancingActivities',\n",
    " 'NetCashProvidedByUsedInFinancingActivitiesContinuingOperations',\n",
    " 'NetCashProvidedByUsedInInvestingActivities',\n",
    " 'NetCashProvidedByUsedInInvestingActivitiesContinuingOperations',\n",
    " 'NetCashProvidedByUsedInOperatingActivities',\n",
    " 'NetCashProvidedByUsedInOperatingActivitiesContinuingOperations',\n",
    " 'NetIncomeLoss',\n",
    " 'OperatingExpenses',\n",
    " 'OperatingIncomeLoss',\n",
    " 'PaymentsForRepurchaseOfCommonStock',\n",
    " 'PaymentsOfDividends',\n",
    " 'PaymentsOfDividendsCommonStock',\n",
    " 'PaymentsOfFinancingCosts',\n",
    " 'PaymentsToAcquireBusinessesNetOfCashAcquired',\n",
    " 'PaymentsToAcquirePropertyPlantAndEquipment',\n",
    " 'RepaymentsOfLongTermDebt',\n",
    " 'ResearchAndDevelopmentExpense',\n",
    " 'Revenues',\n",
    " 'SellingAndMarketingExpense',\n",
    " 'ShareBasedCompensation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+--------------------+--------------------------+---------------+--------------+--------------------------+---------------+--------------+---------------------+-----------------------+-----------------------------+-------------------------------+---------------+---------------------------------+------------------------------------+------------------------------------+-----------------------------+----------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+-------------+-----------------+-------------------+----------------------------------+-------------------+------------------------------+------------------------+--------------------------------------------+------------------------------------------+------------------------+-----------------------------+--------+--------------------------+----------------------+\n",
      "|cik|company_name|assigned_sic|accession_number_int|document_fiscal_year_focus|datapoint_label|statement_type|CostOfGoodsAndServicesSold|CostOfGoodsSold|CostOfServices|EarningsPerShareBasic|EarningsPerShareDiluted|GainLossOnDispositionOfAssets|GeneralAndAdministrativeExpense|IncomeTaxesPaid|IncreaseDecreaseInAccountsPayable|IncreaseDecreaseInAccountsReceivable|IncreaseDecreaseInAccruedLiabilities|IncreaseDecreaseInInventories|LaborAndRelatedExpense|NetCashProvidedByUsedInFinancingActivities|NetCashProvidedByUsedInFinancingActivitiesContinuingOperations|NetCashProvidedByUsedInInvestingActivities|NetCashProvidedByUsedInInvestingActivitiesContinuingOperations|NetCashProvidedByUsedInOperatingActivities|NetCashProvidedByUsedInOperatingActivitiesContinuingOperations|NetIncomeLoss|OperatingExpenses|OperatingIncomeLoss|PaymentsForRepurchaseOfCommonStock|PaymentsOfDividends|PaymentsOfDividendsCommonStock|PaymentsOfFinancingCosts|PaymentsToAcquireBusinessesNetOfCashAcquired|PaymentsToAcquirePropertyPlantAndEquipment|RepaymentsOfLongTermDebt|ResearchAndDevelopmentExpense|Revenues|SellingAndMarketingExpense|ShareBasedCompensation|\n",
      "+---+------------+------------+--------------------+--------------------------+---------------+--------------+--------------------------+---------------+--------------+---------------------+-----------------------+-----------------------------+-------------------------------+---------------+---------------------------------+------------------------------------+------------------------------------+-----------------------------+----------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+-------------+-----------------+-------------------+----------------------------------+-------------------+------------------------------+------------------------+--------------------------------------------+------------------------------------------+------------------------+-----------------------------+--------+--------------------------+----------------------+\n",
      "|  0|           0|         126|                   0|                         0|            175|             0|                    473682|         472084|        475319|               458778|                 459634|                       473792|                         461982|         465629|                           462176|                              457177|                              468231|                       462142|                472099|                                    447288|                                                        466878|                                    449343|                                                        466923|                                    446668|                                                        465862|       442620|           462389|             454013|                            464838|             472378|                        469730|                  472875|                                      468075|                                    452117|                  467375|                       469773|  463809|                    473688|                450651|\n",
      "+---+------------+------------+--------------------+--------------------------+---------------+--------------+--------------------------+---------------+--------------+---------------------+-----------------------+-----------------------------+-------------------------------+---------------+---------------------------------+------------------------------------+------------------------------------+-----------------------------+----------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+-------------+-----------------+-------------------+----------------------------------+-------------------+------------------------------+------------------------+--------------------------------------------+------------------------------------------+------------------------+-----------------------------+--------+--------------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnull(c), c)).alias(c) for c in col]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cik IntegerType\n",
      "company_name StringType\n",
      "assigned_sic IntegerType\n",
      "accession_number_int LongType\n",
      "filing_date DateType\n",
      "document_fiscal_year_focus IntegerType\n",
      "datapoint_label StringType\n",
      "start_date DateType\n",
      "end_date DateType\n",
      "statement_type StringType\n",
      "CostOfGoodsAndServicesSold DoubleType\n",
      "CostOfGoodsSold DoubleType\n",
      "CostOfServices DoubleType\n",
      "EarningsPerShareBasic DoubleType\n",
      "EarningsPerShareDiluted DoubleType\n",
      "GainLossOnDispositionOfAssets DoubleType\n",
      "GeneralAndAdministrativeExpense DoubleType\n",
      "IncomeTaxesPaid DoubleType\n",
      "IncreaseDecreaseInAccountsPayable DoubleType\n",
      "IncreaseDecreaseInAccountsReceivable DoubleType\n",
      "IncreaseDecreaseInAccruedLiabilities DoubleType\n",
      "IncreaseDecreaseInInventories DoubleType\n",
      "LaborAndRelatedExpense DoubleType\n",
      "NetCashProvidedByUsedInFinancingActivities DoubleType\n",
      "NetCashProvidedByUsedInFinancingActivitiesContinuingOperations DoubleType\n",
      "NetCashProvidedByUsedInInvestingActivities DoubleType\n",
      "NetCashProvidedByUsedInInvestingActivitiesContinuingOperations DoubleType\n",
      "NetCashProvidedByUsedInOperatingActivities DoubleType\n",
      "NetCashProvidedByUsedInOperatingActivitiesContinuingOperations DoubleType\n",
      "NetIncomeLoss DoubleType\n",
      "OperatingExpenses DoubleType\n",
      "OperatingIncomeLoss DoubleType\n",
      "PaymentsForRepurchaseOfCommonStock DoubleType\n",
      "PaymentsOfDividends DoubleType\n",
      "PaymentsOfDividendsCommonStock DoubleType\n",
      "PaymentsOfFinancingCosts DoubleType\n",
      "PaymentsToAcquireBusinessesNetOfCashAcquired DoubleType\n",
      "PaymentsToAcquirePropertyPlantAndEquipment DoubleType\n",
      "RepaymentsOfLongTermDebt DoubleType\n",
      "ResearchAndDevelopmentExpense DoubleType\n",
      "Revenues DoubleType\n",
      "SellingAndMarketingExpense DoubleType\n",
      "ShareBasedCompensation DoubleType\n"
     ]
    }
   ],
   "source": [
    "for c in df.columns:\n",
    "    print(c, df.schema[c].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = spark.sql(\"select company_name, document_fiscal_year_focus, max(cik) as cik,\\\n",
    "                  max(assigned_sic) as assigned_sic, max(accession_number_int) as accession_number_int, \\\n",
    "                  max(filing_date) as filing_date, max(CostOfGoodsAndServicesSold) as CostOfGoodsAndServicesSold, \\\n",
    "                  max(CostOfGoodsSold) as CostOfGoodsSold, max(CostOfServices) as CostOfServices, max(EarningsPerShareBasic) as EarningsPerShareBasic, \\\n",
    "                  max(EarningsPerShareDiluted) as EarningsPerShareDiluted, max(GainLossOnDispositionOfAssets) as GainLossOnDispositionOfAssets, \\\n",
    "                  max(GeneralAndAdministrativeExpense) as GeneralAndAdministrativeExpense, max(IncomeTaxesPaid) as IncomeTaxesPaid, \\\n",
    "                  max(IncreaseDecreaseInAccountsPayable) as IncreaseDecreaseInAccountsPayable, max(IncreaseDecreaseInAccountsReceivable) as IncreaseDecreaseInAccountsReceivable, \\\n",
    "                  max(IncreaseDecreaseInAccruedLiabilities) as IncreaseDecreaseInAccruedLiabilities, max(IncreaseDecreaseInInventories) as IncreaseDecreaseInInventories, \\\n",
    "                  max(LaborAndRelatedExpense) as LaborAndRelatedExpense, max(NetCashProvidedByUsedInFinancingActivities) as NetCashProvidedByUsedInFinancingActivities, \\\n",
    "                  max(NetCashProvidedByUsedInFinancingActivitiesContinuingOperations) as NetCashProvidedByUsedInFinancingActivitiesContinuingOperations, \\\n",
    "                  max(NetCashProvidedByUsedInInvestingActivities) as NetCashProvidedByUsedInInvestingActivities, \\\n",
    "                  max(NetCashProvidedByUsedInInvestingActivitiesContinuingOperations) as NetCashProvidedByUsedInInvestingActivitiesContinuingOperations, \\\n",
    "                  max(NetCashProvidedByUsedInOperatingActivities) as NetCashProvidedByUsedInOperatingActivities, \\\n",
    "                  max(NetCashProvidedByUsedInOperatingActivitiesContinuingOperations) as NetCashProvidedByUsedInOperatingActivitiesContinuingOperations, max(NetIncomeLoss) as NetIncomeLoss, \\\n",
    "                  max(OperatingExpenses) as OperatingExpenses, max(OperatingIncomeLoss) as OperatingIncomeLoss, max(PaymentsForRepurchaseOfCommonStock) as PaymentsForRepurchaseOfCommonStock, \\\n",
    "                  max(PaymentsOfDividends) as PaymentsOfDividends, max(PaymentsOfDividendsCommonStock) as PaymentsOfDividendsCommonStock, \\\n",
    "                  max(PaymentsOfFinancingCosts) as PaymentsOfFinancingCosts, max(PaymentsToAcquireBusinessesNetOfCashAcquired) as PaymentsToAcquireBusinessesNetOfCashAcquired, \\\n",
    "                  max(PaymentsToAcquirePropertyPlantAndEquipment) as PaymentsToAcquirePropertyPlantAndEquipment, max(RepaymentsOfLongTermDebt) as RepaymentsOfLongTermDebt, \\\n",
    "                  max(ResearchAndDevelopmentExpense) as ResearchAndDevelopmentExpense, max(Revenues) as Revenues, max(SellingAndMarketingExpense) as SellingAndMarketingExpense, \\\n",
    "                  max(ShareBasedCompensation) as ShareBasedCompensation\\\n",
    "                  from dfTable group by company_name, document_fiscal_year_focus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_merged.withColumn('profit_year', df_merged['document_fiscal_year_focus']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.createOrReplaceTempView(\"dfMergedTable\")\n",
    "df_temp.createOrReplaceTempView(\"dfTempTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp1 = spark.sql(\"select b.cik, b.company_name, b.assigned_sic, b.accession_number_int, b.filing_date, b.CostOfGoodsAndServicesSold, b.CostOfGoodsSold, b.CostOfServices, b.EarningsPerShareBasic, b.EarningsPerShareDiluted, b.GainLossOnDispositionOfAssets, b.GeneralAndAdministrativeExpense, b.IncomeTaxesPaid, b.IncreaseDecreaseInAccountsPayable, b.IncreaseDecreaseInAccountsReceivable, b.IncreaseDecreaseInAccruedLiabilities, b.IncreaseDecreaseInInventories, b.LaborAndRelatedExpense, b.NetCashProvidedByUsedInFinancingActivities, b.NetCashProvidedByUsedInFinancingActivitiesContinuingOperations, b.NetCashProvidedByUsedInInvestingActivities, b.NetCashProvidedByUsedInInvestingActivitiesContinuingOperations, b.NetCashProvidedByUsedInOperatingActivities, b.NetCashProvidedByUsedInOperatingActivitiesContinuingOperations, b.NetIncomeLoss, b.OperatingExpenses, b.OperatingIncomeLoss, b.PaymentsForRepurchaseOfCommonStock, b.PaymentsOfDividends, b.PaymentsOfDividendsCommonStock, b.PaymentsOfFinancingCosts, b.PaymentsToAcquireBusinessesNetOfCashAcquired, b.PaymentsToAcquirePropertyPlantAndEquipment, b.RepaymentsOfLongTermDebt, b.ResearchAndDevelopmentExpense, b.Revenues, b.SellingAndMarketingExpense, b.ShareBasedCompensation, \\\n",
    "                     b.document_fiscal_year_focus as year, b.profit_year, a.OperatingIncomeLoss as profit \\\n",
    "                     from dfMergedTable a inner join dfTempTable b \\\n",
    "                     on a.document_fiscal_year_focus == b.profit_year and a.company_name == b.company_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------+------------+--------------------+-----------+--------------------------+---------------+--------------+---------------------+-----------------------+-----------------------------+-------------------------------+---------------+---------------------------------+------------------------------------+------------------------------------+-----------------------------+----------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+-------------+-----------------+-------------------+----------------------------------+-------------------+------------------------------+------------------------+--------------------------------------------+------------------------------------------+------------------------+-----------------------------+----------+--------------------------+----------------------+----+-----------+----------+\n",
      "|cik    |company_name                         |assigned_sic|accession_number_int|filing_date|CostOfGoodsAndServicesSold|CostOfGoodsSold|CostOfServices|EarningsPerShareBasic|EarningsPerShareDiluted|GainLossOnDispositionOfAssets|GeneralAndAdministrativeExpense|IncomeTaxesPaid|IncreaseDecreaseInAccountsPayable|IncreaseDecreaseInAccountsReceivable|IncreaseDecreaseInAccruedLiabilities|IncreaseDecreaseInInventories|LaborAndRelatedExpense|NetCashProvidedByUsedInFinancingActivities|NetCashProvidedByUsedInFinancingActivitiesContinuingOperations|NetCashProvidedByUsedInInvestingActivities|NetCashProvidedByUsedInInvestingActivitiesContinuingOperations|NetCashProvidedByUsedInOperatingActivities|NetCashProvidedByUsedInOperatingActivitiesContinuingOperations|NetIncomeLoss|OperatingExpenses|OperatingIncomeLoss|PaymentsForRepurchaseOfCommonStock|PaymentsOfDividends|PaymentsOfDividendsCommonStock|PaymentsOfFinancingCosts|PaymentsToAcquireBusinessesNetOfCashAcquired|PaymentsToAcquirePropertyPlantAndEquipment|RepaymentsOfLongTermDebt|ResearchAndDevelopmentExpense|Revenues  |SellingAndMarketingExpense|ShareBasedCompensation|year|profit_year|profit    |\n",
      "+-------+-------------------------------------+------------+--------------------+-----------+--------------------------+---------------+--------------+---------------------+-----------------------+-----------------------------+-------------------------------+---------------+---------------------------------+------------------------------------+------------------------------------+-----------------------------+----------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+-------------+-----------------+-------------------+----------------------------------+-------------------+------------------------------+------------------------+--------------------------------------------+------------------------------------------+------------------------+-----------------------------+----------+--------------------------+----------------------+----+-----------+----------+\n",
      "|318306 |ABEONA THERAPEUTICS INC.             |2834        |31830613000004      |2013-03-20 |null                      |267000.0       |null          |null                 |null                   |null                         |null                           |null           |null                             |null                                |null                                |43000.0                      |null                  |1904000.0                                 |null                                                          |-13000.0                                  |null                                                          |-3955000.0                                |null                                                          |-1.0532E7    |null             |-4316000.0         |null                              |null               |null                          |null                    |null                                        |13000.0                                   |null                    |2010000.0                    |null      |null                      |390000.0              |2012|2013       |-3804000.0|\n",
      "|1590496|AERKOMM INC.                         |4899        |147793216008603     |2016-02-16 |null                      |6486.0         |null          |null                 |null                   |null                         |null                           |null           |null                             |null                                |-1131.0                             |null                         |null                  |13000.0                                   |null                                                          |null                                      |null                                                          |-37710.0                                  |null                                                          |-36579.0     |40145.0          |-36579.0           |null                              |null               |null                          |null                    |null                                        |null                                      |null                    |null                         |null      |null                      |null                  |2015|2016       |-24613.0  |\n",
      "|1004434|AFFILIATED MANAGERS GROUP, INC.      |6282        |104746913001515     |2013-02-22 |null                      |null           |null          |3.36                 |3.28                   |null                         |null                           |null           |null                             |3.48E7                              |null                                |null                         |7.847E8               |null                                      |1.462E8                                                       |null                                      |-8.023E8                                                      |null                                      |6.332E8                                                       |1.74E8       |1.4051E9         |4.004E8            |6.09E7                            |null               |null                          |1.04E7                  |null                                        |2.0E7                                     |null                    |null                         |null      |null                      |4.76E7                |2012|2013       |6.341E8   |\n",
      "|897077 |ALAMO GROUP INC                      |3523        |89707713000009      |2013-03-11 |4.8489E8                  |null           |null          |2.43                 |2.4                    |null                         |null                           |null           |null                             |-4770000.0                          |null                                |-6932000.0                   |null                  |-9942000.0                                |null                                                          |-4090000.0                                |null                                                          |5.1263E7                                  |null                                                          |2.8903E7     |null             |4.5349E7           |null                              |null               |2854000.0                     |null                    |0.0                                         |4654000.0                                 |null                    |null                         |null      |null                      |940000.0              |2012|2013       |5.0737E7  |\n",
      "|1124804|ALLSCRIPTS HEALTHCARE SOLUTIONS, INC.|7373        |156459017002448     |2017-02-27 |null                      |null           |4.59174E8     |-0.14                |-0.14                  |null                         |null                           |null           |4.0456E7                         |1.7826E7                            |1490000.0                           |null                         |null                  |null                                      |8.65271E8                                                     |null                                      |-1.154006E9                                                   |null                                      |2.69004E8                                                     |-2.5652E7    |null             |5.9771E7           |1.21241E8                         |null               |null                          |null                    |null                                        |3.551E7                                   |null                    |1.87906E8                    |1.549899E9|null                      |4.2877E7              |2016|2017       |4.1917E7  |\n",
      "+-------+-------------------------------------+------------+--------------------+-----------+--------------------------+---------------+--------------+---------------------+-----------------------+-----------------------------+-------------------------------+---------------+---------------------------------+------------------------------------+------------------------------------+-----------------------------+----------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+------------------------------------------+--------------------------------------------------------------+-------------+-----------------+-------------------+----------------------------------+-------------------+------------------------------+------------------------+--------------------------------------------+------------------------------------------+------------------------+-----------------------------+----------+--------------------------+----------------------+----+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp1.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31916"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cik IntegerType\n",
      "company_name StringType\n",
      "assigned_sic IntegerType\n",
      "accession_number_int LongType\n",
      "filing_date DateType\n",
      "CostOfGoodsAndServicesSold DoubleType\n",
      "CostOfGoodsSold DoubleType\n",
      "CostOfServices DoubleType\n",
      "EarningsPerShareBasic DoubleType\n",
      "EarningsPerShareDiluted DoubleType\n",
      "GainLossOnDispositionOfAssets DoubleType\n",
      "GeneralAndAdministrativeExpense DoubleType\n",
      "IncomeTaxesPaid DoubleType\n",
      "IncreaseDecreaseInAccountsPayable DoubleType\n",
      "IncreaseDecreaseInAccountsReceivable DoubleType\n",
      "IncreaseDecreaseInAccruedLiabilities DoubleType\n",
      "IncreaseDecreaseInInventories DoubleType\n",
      "LaborAndRelatedExpense DoubleType\n",
      "NetCashProvidedByUsedInFinancingActivities DoubleType\n",
      "NetCashProvidedByUsedInFinancingActivitiesContinuingOperations DoubleType\n",
      "NetCashProvidedByUsedInInvestingActivities DoubleType\n",
      "NetCashProvidedByUsedInInvestingActivitiesContinuingOperations DoubleType\n",
      "NetCashProvidedByUsedInOperatingActivities DoubleType\n",
      "NetCashProvidedByUsedInOperatingActivitiesContinuingOperations DoubleType\n",
      "NetIncomeLoss DoubleType\n",
      "OperatingExpenses DoubleType\n",
      "OperatingIncomeLoss DoubleType\n",
      "PaymentsForRepurchaseOfCommonStock DoubleType\n",
      "PaymentsOfDividends DoubleType\n",
      "PaymentsOfDividendsCommonStock DoubleType\n",
      "PaymentsOfFinancingCosts DoubleType\n",
      "PaymentsToAcquireBusinessesNetOfCashAcquired DoubleType\n",
      "PaymentsToAcquirePropertyPlantAndEquipment DoubleType\n",
      "RepaymentsOfLongTermDebt DoubleType\n",
      "ResearchAndDevelopmentExpense DoubleType\n",
      "Revenues DoubleType\n",
      "SellingAndMarketingExpense DoubleType\n",
      "ShareBasedCompensation DoubleType\n",
      "year IntegerType\n",
      "profit_year IntegerType\n",
      "profit DoubleType\n"
     ]
    }
   ],
   "source": [
    "for c in df_temp1.columns:\n",
    "    print(c, df_temp1.schema[c].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp1.write.parquet(\"s3://502-project-1/data\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp1 = spark.read.parquet(\"s3://502-project-1/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmlspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31916"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem',\n",
       "  '2'),\n",
       " ('spark.executor.instances', '4'),\n",
       " ('spark.sql.execution.arrow.enabled', 'true'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'ip-172-31-46-208.ec2.internal'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native'),\n",
       " ('spark.sql.parquet.output.committer.class',\n",
       "  'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter'),\n",
       " ('spark.blacklist.decommissioning.timeout', '1h'),\n",
       " ('spark.driver.port', '36195'),\n",
       " ('spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS', '$(hostname -f)'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.sql.emr.internal.extensions',\n",
       "  'com.amazonaws.emr.spark.EmrSparkSessionExtensions'),\n",
       " ('spark.executor.memory', '10356M'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\"),\n",
       " ('spark.app.id', 'application_1587406509124_0004'),\n",
       " ('spark.eventLog.dir', 'hdfs:///var/log/spark/apps'),\n",
       " ('spark.driver.memory', '11171M'),\n",
       " ('spark.sql.hive.metastore.sharedPrefixes',\n",
       "  'com.amazonaws.services.dynamodbv2'),\n",
       " ('spark.sql.warehouse.dir', 'hdfs:///user/spark/warehouse'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.default.parallelism', '32'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///var/log/spark/apps'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true'),\n",
       " ('spark.driver.extraClassPath',\n",
       "  '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'),\n",
       " ('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem',\n",
       "  'true'),\n",
       " ('spark.history.ui.port', '18080'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.yarn.historyServer.address', 'ip-172-31-46-208.ec2.internal:18080'),\n",
       " ('spark.emr.maximizeResourceAllocation', 'true'),\n",
       " ('spark.hadoop.yarn.timeline-service.enabled', 'false'),\n",
       " ('spark.resourceManager.cleanupExpiredHost', 'true'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.extraClassPath',\n",
       "  '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'),\n",
       " ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  \"-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\"),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://ip-172-31-46-208.ec2.internal:20888/proxy/application_1587406509124_0004'),\n",
       " ('spark.app.name', 'abc'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.driver.host', 'ip-172-31-46-208.ec2.internal'),\n",
       " ('spark.decommissioning.timeout.threshold', '20'),\n",
       " ('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1587406509124_0001'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native'),\n",
       " ('spark.driver.appUIAddress', 'http://ip-172-31-46-208.ec2.internal:4040'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000'),\n",
       " ('spark.blacklist.decommissioning.enabled', 'true')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o330.count.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:187)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:40)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:526)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:662)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:166)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:238)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:164)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:40)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:283)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:332)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(company_name#5410, document_fiscal_year_focus#5414, 200)\n+- *(2) HashAggregate(keys=[company_name#5410, document_fiscal_year_focus#5414], functions=[], output=[company_name#5410, document_fiscal_year_focus#5414])\n   +- *(2) Filter (isnotnull(company_name#5410) && isnotnull((document_fiscal_year_focus#5414 + 1)))\n      +- InMemoryTableScan [company_name#5410, document_fiscal_year_focus#5414], [isnotnull(company_name#5410), isnotnull((document_fiscal_year_focus#5414 + 1))]\n            +- InMemoryRelation [cik#5409, company_name#5410, assigned_sic#5411, accession_number_int#5412L, filing_date#5413, document_fiscal_year_focus#5414, datapoint_label#5415, start_date#5416, end_date#5417, statement_type#5418, segment_label#5419, segment_hash#5420, CostOfGoodsAndServicesSold#5421, CostOfGoodsSold#5422, CostOfServices#5423, EarningsPerShareBasic#5424, EarningsPerShareDiluted#5425, GainLossOnDispositionOfAssets#5426, GeneralAndAdministrativeExpense#5427, IncomeTaxesPaid#5428, IncreaseDecreaseInAccountsPayable#5429, IncreaseDecreaseInAccountsReceivable#5430, IncreaseDecreaseInAccruedLiabilities#5431, IncreaseDecreaseInInventories#5432, ... 21 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n                  +- *(1) FileScan parquet [cik#0,company_name#1,assigned_sic#2,accession_number_int#3L,filing_date#4,document_fiscal_year_focus#5,datapoint_label#6,start_date#7,end_date#8,statement_type#9,segment_label#10,segment_hash#11,CostOfGoodsAndServicesSold#12,CostOfGoodsSold#13,CostOfServices#14,EarningsPerShareBasic#15,EarningsPerShareDiluted#16,GainLossOnDispositionOfAssets#17,GeneralAndAdministrativeExpense#18,IncomeTaxesPaid#19,IncreaseDecreaseInAccountsPayable#20,IncreaseDecreaseInAccountsReceivable#21,IncreaseDecreaseInAccruedLiabilities#22,IncreaseDecreaseInInventories#23,... 21 more fields] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://sec-finc/pivot_data_vpq], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cik:int,company_name:string,assigned_sic:int,accession_number_int:bigint,filing_date:date,...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:283)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:342)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:79)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withExecutionId$1.apply(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.needToCopyObjectsBeforeShuffle(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:305)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 30 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-f5ef743fe484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_temp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o330.count.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:211)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:101)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:187)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:40)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:526)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:662)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:166)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:40)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithoutKeys(HashAggregateExec.scala:238)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:164)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:40)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:283)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:332)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(company_name#5410, document_fiscal_year_focus#5414, 200)\n+- *(2) HashAggregate(keys=[company_name#5410, document_fiscal_year_focus#5414], functions=[], output=[company_name#5410, document_fiscal_year_focus#5414])\n   +- *(2) Filter (isnotnull(company_name#5410) && isnotnull((document_fiscal_year_focus#5414 + 1)))\n      +- InMemoryTableScan [company_name#5410, document_fiscal_year_focus#5414], [isnotnull(company_name#5410), isnotnull((document_fiscal_year_focus#5414 + 1))]\n            +- InMemoryRelation [cik#5409, company_name#5410, assigned_sic#5411, accession_number_int#5412L, filing_date#5413, document_fiscal_year_focus#5414, datapoint_label#5415, start_date#5416, end_date#5417, statement_type#5418, segment_label#5419, segment_hash#5420, CostOfGoodsAndServicesSold#5421, CostOfGoodsSold#5422, CostOfServices#5423, EarningsPerShareBasic#5424, EarningsPerShareDiluted#5425, GainLossOnDispositionOfAssets#5426, GeneralAndAdministrativeExpense#5427, IncomeTaxesPaid#5428, IncreaseDecreaseInAccountsPayable#5429, IncreaseDecreaseInAccountsReceivable#5430, IncreaseDecreaseInAccruedLiabilities#5431, IncreaseDecreaseInInventories#5432, ... 21 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n                  +- *(1) FileScan parquet [cik#0,company_name#1,assigned_sic#2,accession_number_int#3L,filing_date#4,document_fiscal_year_focus#5,datapoint_label#6,start_date#7,end_date#8,statement_type#9,segment_label#10,segment_hash#11,CostOfGoodsAndServicesSold#12,CostOfGoodsSold#13,CostOfServices#14,EarningsPerShareBasic#15,EarningsPerShareDiluted#16,GainLossOnDispositionOfAssets#17,GeneralAndAdministrativeExpense#18,IncomeTaxesPaid#19,IncreaseDecreaseInAccountsPayable#20,IncreaseDecreaseInAccountsReceivable#21,IncreaseDecreaseInAccruedLiabilities#22,IncreaseDecreaseInInventories#23,... 21 more fields] Batched: true, Format: Parquet, Location: InMemoryFileIndex[s3://sec-finc/pivot_data_vpq], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cik:int,company_name:string,assigned_sic:int,accession_number_int:bigint,filing_date:date,...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:391)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:627)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:283)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:342)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:79)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:76)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withExecutionId$1.apply(SQLExecution.scala:141)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:75)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.needToCopyObjectsBeforeShuffle(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:305)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:91)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 30 more\n"
     ]
    }
   ],
   "source": [
    "df_temp1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
